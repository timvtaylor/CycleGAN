{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5wibaOxTFYa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import os\n",
        "from torchvision.io import decode_image\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv2Zkm7i8Tkj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec2uTRckJf1s"
      },
      "outputs": [],
      "source": [
        "# 256 channel residual block\n",
        "class R256(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(R256, self).__init__()\n",
        "    # Convolution block\n",
        "    # In: [B, 256, H, W]\n",
        "    # Out: [N, 256, H, W]\n",
        "    self.conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, bias=False, padding='same', padding_mode='reflect')\n",
        "    self.norm1 = nn.InstanceNorm2d(num_features=256)\n",
        "    self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, bias=False, padding='same', padding_mode='reflect')\n",
        "    self.norm2 = nn.InstanceNorm2d(num_features=256)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.norm1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.norm2(out)\n",
        "    # skip connection\n",
        "    out = out + x\n",
        "    out = self.relu(out)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCxIzOUmh7ZE"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(generator, self).__init__()\n",
        "    # Convolution block\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out: [B, 64, H, W]\n",
        "    self.c7s1_64 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding='same', padding_mode='reflect'),\n",
        "        nn.InstanceNorm2d(num_features=64),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "    # Downsampling block\n",
        "    # In: [B, 64, H, W]\n",
        "    # Out: [B, 128, H/2, W/2]\n",
        "    self.d128 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(num_features=128),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    # Downsampling block\n",
        "    # In: [B, 128, H/2, W/2]\n",
        "    # Out: [B, 256, H/4, W/4]\n",
        "    self.d256 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(num_features=256),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    # Residual block\n",
        "    # In: [B, 256, H/4, W/4]\n",
        "    # Out: [B, 256, H/4, W/4]\n",
        "    self.R256x9 = nn.Sequential(\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256(),\n",
        "        R256()\n",
        "    )\n",
        "    # Upsampling block\n",
        "    # In: [B, 256, H/4, W/4]\n",
        "    # Out: [B, 128, H/2, W/2]\n",
        "    self.u128 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.InstanceNorm2d(num_features=128),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    # Upsampling block\n",
        "    # In: [B, 128, H/2, W/2]\n",
        "    # Out: [B, 64, H, W]\n",
        "    self.u64 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.InstanceNorm2d(num_features=64),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    # Convolution block\n",
        "    # In: [B, 64, H, W]\n",
        "    # Out: [B, 3, H, W]\n",
        "    self.c7s1_3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding='same', padding_mode='reflect'),\n",
        "        nn.InstanceNorm2d(num_features=3),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.c7s1_64(x)\n",
        "    out = self.d128(out)\n",
        "    out = self.d256(out)\n",
        "    out = self.R256x9(out)\n",
        "    out = self.u128(out)\n",
        "    out = self.u64(out)\n",
        "    out = self.c7s1_3(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ7ex5BlniEJ"
      },
      "outputs": [],
      "source": [
        "# Discriminator Class\n",
        "class discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(discriminator, self).__init__()\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out:\n",
        "    self.c64 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.LeakyReLU(negative_slope=0.2)\n",
        "    )\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out:\n",
        "    self.c128 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(num_features=128),\n",
        "        nn.LeakyReLU(negative_slope=0.2)\n",
        "    )\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out:\n",
        "    self.c256 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(num_features=256),\n",
        "        nn.LeakyReLU(negative_slope=0.2)\n",
        "    )\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out:\n",
        "    self.c512 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1),\n",
        "        nn.InstanceNorm2d(num_features=512),\n",
        "        nn.LeakyReLU(negative_slope=0.2)\n",
        "    )\n",
        "    # In: [B, 3, H, W]\n",
        "    # Out:\n",
        "    self.final = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1)\n",
        "  def forward(self, x):\n",
        "    out = self.c64(x)\n",
        "    out = self.c128(out)\n",
        "    out = self.c256(out)\n",
        "    out = self.c512(out)\n",
        "    out = self.final(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v841YH_goSBM"
      },
      "outputs": [],
      "source": [
        "# Generative Adverserial Model\n",
        "class GAN(nn.Module):\n",
        "  def __init__(self, discriminator, generator):\n",
        "    super(GAN, self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.generator(x)\n",
        "    out = self.discriminator(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbaulP9FJoCn"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, img_paths, transform):\n",
        "      self.img_paths = img_paths\n",
        "      self.transform = transform\n",
        "    def __len__(self):\n",
        "      return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "      img_path = self.img_paths[idx]\n",
        "      image = decode_image(img_path)\n",
        "      image = self.transform(image)\n",
        "      return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWecxC-Z9-Fd"
      },
      "outputs": [],
      "source": [
        "def get_img_paths(img_directory):\n",
        "  image_paths = []\n",
        "  for filename in os.listdir(img_directory):\n",
        "      if filename.lower().endswith(('.jpg', '.jpeg')):\n",
        "        fullA_path = os.path.join(img_directory, filename)\n",
        "        image_paths.append(fullA_path)\n",
        "  return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMybSsKplZ9a"
      },
      "outputs": [],
      "source": [
        "def plot_image_and_translation(generator, image, img_dim):\n",
        "  denormalize = transforms.Normalize(\n",
        "    mean=[-1, -1, -1],\n",
        "    std=[2, 2, 2]\n",
        "  )\n",
        "  with torch.no_grad():\n",
        "    translation = generator(image).cpu()\n",
        "    translation = denormalize(translation).numpy()\n",
        "    translation = translation.reshape((3, img_dim, img_dim))\n",
        "    translation = translation.transpose(1, 2, 0)\n",
        "  image = image.detach().cpu()\n",
        "  image = denormalize(image).numpy()\n",
        "  image = image.reshape((3, img_dim, img_dim))\n",
        "  image = image.transpose(1, 2, 0)\n",
        "\n",
        "  plt.figure(figsize=(6,3))\n",
        "  ax = plt.subplot(1, 2, 1)\n",
        "  ax.axis('off')\n",
        "  plt.imshow(image)\n",
        "  ax = plt.subplot(1, 2, 2)\n",
        "  ax.axis('off')\n",
        "  plt.imshow(translation)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAWBVqtHBoZB"
      },
      "outputs": [],
      "source": [
        "# Instantiate datasets\n",
        "denormalize = transforms.Normalize(\n",
        "    mean=[-1, -1, -1],\n",
        "    std=[2, 2, 2]\n",
        ")\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), # Resize to 128x128\n",
        "    transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Convert to PyTorch Tensor\n",
        "    transforms.Lambda(lambda x: x[:3, :, :]), # Convert to RGB by selecting the first 3 channels\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize\n",
        "])\n",
        "\n",
        "# Instatiate landscape dataset\n",
        "A_paths = get_img_paths('/content/drive/MyDrive/CycleGAN/domainA/trainA/')\n",
        "A_dataset = ImageDataset(A_paths, transform)\n",
        "# Visualize first 4 images\n",
        "for i, image in enumerate(A_dataset):\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    ax.axis('off')\n",
        "    plt.imshow(denormalize(image).permute(1, 2, 0)) # permute to make it a numpy image from tensor\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsiKYxeLCrLf"
      },
      "outputs": [],
      "source": [
        "# Instantiate datasets\n",
        "\n",
        "# Denormalization for visualizing the image\n",
        "denormalize = transforms.Normalize(\n",
        "    mean=[-1, -1, -1],\n",
        "    std=[2, 2, 2]\n",
        ")\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), # Resize to 128x128\n",
        "    transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Convert to PyTorch Tensor\n",
        "    transforms.Lambda(lambda x: x[:3, :, :]), # Convert to RGB by selecting the first 3 channels\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize\n",
        "])\n",
        "\n",
        "# Instatiate landscape dataset\n",
        "B_paths = get_img_paths('/content/drive/MyDrive/CycleGAN/domainB/trainB/')\n",
        "B_dataset = ImageDataset(B_paths, transform)\n",
        "# Visualize first 4 images\n",
        "for i, image in enumerate(B_dataset):\n",
        "    ax = plt.subplot(1, 4, i + 1)\n",
        "    ax.axis('off')\n",
        "    plt.imshow(denormalize(image).permute(1, 2, 0)) # permute image dims to make it (H, W, C) from (C, H, W)\n",
        "\n",
        "    if i == 3:\n",
        "        plt.show()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgMP4HCoDUh3"
      },
      "outputs": [],
      "source": [
        "# Instantiate dataloaders\n",
        "batch_size = 1\n",
        "A_dataloader = DataLoader(A_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "B_dataloader = DataLoader(B_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r20D_sqoMH8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMVj1NUA_F3J"
      },
      "outputs": [],
      "source": [
        "## Instantiate models, optimizers, and loss functions\n",
        "\n",
        "# Instatiate Generator_BA\n",
        "Generator_BA = generator().to(device)\n",
        "PL_state_dict = torch.load('/content/drive/MyDrive/CycleGAN/models/Generator_BA_e130.pth', map_location=torch.device(device))\n",
        "Generator_BA.load_state_dict(PL_state_dict)\n",
        "# Instantaite Generator_AB\n",
        "Generator_AB = generator().to(device)\n",
        "LP_state_dict = torch.load('/content/drive/MyDrive/CycleGAN/models/Generator_AB_e130.pth', map_location=torch.device(device))\n",
        "Generator_AB.load_state_dict(LP_state_dict)\n",
        "# Instantaite DiscriminatorL\n",
        "Discriminator_A = discriminator().to(device)\n",
        "L_state_dict = torch.load('/content/drive/MyDrive/CycleGAN/models/Discriminator_A_e130.pth', map_location=torch.device(device))\n",
        "Discriminator_A.load_state_dict(L_state_dict)\n",
        "#Instantaite DiscriminatorP\n",
        "Discriminator_B = discriminator().to(device)\n",
        "P_state_dict = torch.load('/content/drive/MyDrive/CycleGAN/models/Discriminator_B_e130.pth', map_location=torch.device(device))\n",
        "Discriminator_B.load_state_dict(P_state_dict)\n",
        "# Instantiate GAN_BA\n",
        "GAN_BA = GAN(Discriminator_A, Generator_BA)\n",
        "# Instantiate GAN_AB\n",
        "GAN_AB = GAN(Discriminator_B, Generator_AB)\n",
        "\n",
        "# Optimizer for the GANs\n",
        "optimizer_GANs = torch.optim.Adam(\n",
        "    chain(Generator_BA.parameters(), Generator_AB.parameters()), lr=0.0002, betas=(0.5, 0.999)\n",
        ")\n",
        "\n",
        "# Optimizer for Discriminator_A\n",
        "optimizer_disc_A = torch.optim.Adam(Discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Optimizer for Discriminator_B\n",
        "optimizer_disc_B = torch.optim.Adam(Discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Criterion for GANs\n",
        "criterion_GANs = nn.MSELoss()\n",
        "\n",
        "# Criterion for cycle consistency\n",
        "criterion_cycle = nn.L1Loss()\n",
        "\n",
        "# Criterion for identity\n",
        "criterion_id = nn.L1Loss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw8CXfwa997L"
      },
      "outputs": [],
      "source": [
        "def train(A_dataloader,\n",
        "    B_dataloader,\n",
        "    Generator_BA,\n",
        "    Generator_AB,\n",
        "    Discriminator_A,\n",
        "    Discriminator_B,\n",
        "    GAN_BA,\n",
        "    GAN_AB,\n",
        "    optimizer_disc_A,\n",
        "    optimizer_disc_B,\n",
        "    criterion_GANs,\n",
        "    criterion_cycle,\n",
        "    criterion_id,\n",
        "          epoch):\n",
        "  lambda_cycle = 10\n",
        "  lambda_identity = 0.1*lambda_cycle\n",
        "  i = 0\n",
        "  for real_image_A, real_image_B in zip(A_dataloader, B_dataloader):\n",
        "    ## GET REAL AND FAKE IMAGES\n",
        "    ## ------------------------\n",
        "    real_image_A = real_image_A.to(device)\n",
        "    real_image_B = real_image_B.to(device)\n",
        "    fake_image_A = Generator_BA(real_image_B)\n",
        "    fake_image_B = Generator_AB(real_image_A)\n",
        "    ## TRAIN DISCRIMINATOR A\n",
        "    ## ---------------------\n",
        "    # Zero the gradient for Discriminator A's optimizer\n",
        "    optimizer_disc_A.zero_grad()\n",
        "    # Calculate predictions and loss for Discriminator A on real images from domain A\n",
        "    pred_dA_real = Discriminator_A(real_image_A)\n",
        "    loss_dA_real = criterion_GANs(pred_dA_real, torch.ones_like(pred_dA_real).to(device))\n",
        "    # Calculate predictions and loss for Discriminator A on translated images (B to A)\n",
        "    pred_dA_fake = Discriminator_A(fake_image_A.detach())\n",
        "    loss_dA_fake = criterion_GANs(pred_dA_fake, torch.zeros_like(pred_dA_fake).to(device))\n",
        "    loss_disc_A = 0.5*(loss_dA_real+loss_dA_fake)\n",
        "    loss_disc_A.backward()\n",
        "    optimizer_disc_A.step()\n",
        "\n",
        "    ## TRAIN DISCRIMINATOR B\n",
        "    ## ---------------------\n",
        "    optimizer_disc_B.zero_grad()\n",
        "    pred_dB_real = Discriminator_B(real_image_B)\n",
        "    loss_dB_real = criterion_GANs(pred_dB_real, torch.ones_like(pred_dB_real).to(device))\n",
        "    pred_dB_fake = Discriminator_B(fake_image_B.detach())\n",
        "    loss_dB_fake = criterion_GANs(pred_dB_fake, torch.zeros_like(pred_dB_fake).to(device))\n",
        "    loss_disc_B = 0.5*(loss_dB_real+loss_dB_fake)\n",
        "    loss_disc_B.backward()\n",
        "    optimizer_disc_B.step()\n",
        "\n",
        "    ## TRAIN GENERATORS\n",
        "    ## ----------------\n",
        "    optimizer_GANs.zero_grad()\n",
        "    # Adversarial loss\n",
        "    pred_GAN_AB = GAN_AB(real_image_A)\n",
        "    pred_GAN_BA = GAN_BA(real_image_B)\n",
        "    adv_loss_GAN_AB = criterion_GANs(pred_GAN_AB, torch.ones_like(pred_GAN_AB).to(device))\n",
        "    adv_loss_GAN_BA = criterion_GANs(pred_GAN_BA, torch.ones_like(pred_GAN_BA).to(device))\n",
        "    # Cycle consistency loss\n",
        "    recovered_image_A = Generator_BA(fake_image_B)\n",
        "    recovered_image_B = Generator_AB(fake_image_A)\n",
        "    cycle_loss_AB = criterion_cycle(recovered_image_B, real_image_B)\n",
        "    cycle_loss_BA = criterion_cycle(recovered_image_A, real_image_A)\n",
        "    # Identity loss\n",
        "    identity_image_B = Generator_AB(real_image_B)\n",
        "    identity_image_A = Generator_BA(real_image_A)\n",
        "    identity_loss_B = criterion_id(real_image_B, identity_image_B)\n",
        "    identity_loss_A = criterion_id(real_image_A, identity_image_A)\n",
        "    GAN_loss = (adv_loss_GAN_AB + adv_loss_GAN_BA + lambda_cycle*(cycle_loss_AB + cycle_loss_BA)\n",
        "                + lambda_identity*(identity_loss_A + identity_loss_B))\n",
        "    GAN_loss.backward()\n",
        "    optimizer_GANs.step()\n",
        "    if (epoch%10==0 and i%1000==1):\n",
        "      plot_image_and_translation(generator=Generator_AB, image=real_image_A, img_dim=256)\n",
        "      plot_image_and_translation(generator=Generator_BA, image=real_image_B, img_dim=256)\n",
        "      AB_path = f\"/content/drive/MyDrive/CycleGAN/models/Generator_AB_e{epoch+90}.pth\"\n",
        "      BA_path = f\"/content/drive/MyDrive/CycleGAN/models/Generator_BA_e{epoch+90}.pth\"\n",
        "      A_path = f\"/content/drive/MyDrive/CycleGAN/models/Discriminator_A_e{epoch+90}.pth\"\n",
        "      B_path = f\"/content/drive/MyDrive/CycleGAN/models/Discriminator_B_e{epoch+90}.pth\"\n",
        "      torch.save(Generator_AB.state_dict(), AB_path)\n",
        "      torch.save(Generator_BA.state_dict(), BA_path)\n",
        "      torch.save(Discriminator_A.state_dict(), A_path)\n",
        "      torch.save(Discriminator_B.state_dict(), B_path)\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WK9dcxL_puf"
      },
      "outputs": [],
      "source": [
        "num_epochs = 201\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch: {90+epoch}\")\n",
        "  print('-------------------')\n",
        "  train(A_dataloader,\n",
        "    B_dataloader,\n",
        "    Generator_BA,\n",
        "    Generator_AB,\n",
        "    Discriminator_A,\n",
        "    Discriminator_B,\n",
        "    GAN_BA,\n",
        "    GAN_AB,\n",
        "    optimizer_disc_A,\n",
        "    optimizer_disc_B,\n",
        "    criterion_GANs,\n",
        "    criterion_cycle,\n",
        "    criterion_id,\n",
        "        epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I9J1brjLGNs"
      },
      "outputs": [],
      "source": [
        "# Performance on fullsized images\n",
        "denormalize = transforms.Normalize(\n",
        "    mean=[-1, -1, -1],\n",
        "    std=[2, 2, 2]\n",
        ")\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize((256, 256)), # Resize to 128x128\n",
        "    transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]), # Convert to PyTorch Tensor\n",
        "    transforms.Lambda(lambda x: x[:3, :, :]), # Convert to RGB by selecting the first 3 channels\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize\n",
        "])\n",
        "monet_benchmark = decode_image('/content/drive/MyDrive/CycleGAN/images/monet_benchmark.jpg')\n",
        "landscape_benchmark = decode_image('/content/drive/MyDrive/CycleGAN/images/landscape_benchmark3.jpg')\n",
        "monet_benchmark = transform(monet_benchmark)\n",
        "landscape_benchmark = transform(landscape_benchmark)\n",
        "monet_translation = Generator_AB(monet_benchmark)\n",
        "landscape_translation = Generator_BA(landscape_benchmark)\n",
        "landscape_benchmark = denormalize(landscape_benchmark).permute(1,2,0)\n",
        "monet_benchmark = denormalize(monet_benchmark).permute(1,2,0)\n",
        "landscape_translation = denormalize(landscape_translation).permute(1,2,0).detach()\n",
        "monet_translation = denormalize(monet_translation).permute(1,2,0).detach()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(monet_benchmark)\n",
        "plt.title('Painting')\n",
        "plt.savefig('/content/drive/MyDrive/CycleGAN/images/monet_original.jpg')"
      ],
      "metadata": {
        "id": "5-v-DkYXaTEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(monet_translation)\n",
        "plt.title('Translation Into Photograph')\n",
        "plt.savefig('/content/drive/MyDrive/CycleGAN/images/monet_translation_to_photo.jpg')\n"
      ],
      "metadata": {
        "id": "mjTNRLBKbIbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(landscape_benchmark)\n",
        "plt.title('Photograph')\n",
        "plt.savefig('/content/drive/MyDrive/CycleGAN/images/landscape_original.jpg')"
      ],
      "metadata": {
        "id": "u8W77jxBbmOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(landscape_translation)\n",
        "plt.title('Translation into Painting')\n",
        "plt.savefig('/content/drive/MyDrive/CycleGAN/images/photo_translation_to_painting.jpg')"
      ],
      "metadata": {
        "id": "OpFU9zlcbvZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}